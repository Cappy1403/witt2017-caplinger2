---
title: 'Machine Learning Algorithms in R'
subtitle: 'Collection of Scripts For Many ML Algorithms'
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

## Read data

```{r}
dats <- read.csv('../data/Sub.csv')
head(dats, 100)
dats$EMPSTAT <- as.factor(dats$EMPSTAT) 
dats$OWNERSHP <- as.factor(dats$OWNERSHP) 
dats$MARST <- as.factor(dats$MARST) 
dats$HCOVANY <- as.factor(dats$HCOVANY) 
dats$REGION <- as.factor(dats$REGION)
dats$METRO <- as.factor(dats$METRO)
dats$STATEFIP <- as.factor(dats$STATEFIP)
```

First, let's create a training dataset and a testing dataset. The training data set will be some fraction $(100\times p\%)$ of the total number of observations in the full dataset.  We can create the training and test datasets by sampling $p \times \mbox{number of rows}$ rows from the full dataset.  A common fractional value used to create the training dataset is $70\%$.

```{r}
p <- 0.7
set.seed(42)
train_rows <- sample(nrow(dats), size = p*nrow(dats))
test_rows  <- which(!1:nrow(dats)%in%train_rows)
```

Next, we need to identify and parse out the columns representing the features and response variable(s) of interest.

```{r}
features <- c("FAMSIZE","OWNERSHP","MARST","HCOVANY","EMPSTAT",
              "METRO","REGION")
response <- c("POVERTY")
```

Now, we can create the datasets

```{r}
x_train <- dats[train_rows, features]
y_train <- dats[train_rows, response]
x_test  <- dats[test_rows,  features]
y_test  <- dats[test_rows,  response]
train_data <- cbind(x_train, y_train)
test_data  <- cbind(x_test, y_test)
```

## Linear Regression

Train the model using the training sets and check score

```{r}
linear <- lm(y_train ~ ., data = train_data)

summary(linear)

predicted_lin = predict(linear, x_test)

press <- sqrt(sum((y_test - predicted_lin)^2))/nrow(x_test)
```

## Logistic Regression

Train the model using the training sets and check score

```{r}
Y_train    <- as.numeric(y_train > 100)
Train_Data <- cbind(x_train, Y_train)

logistic   <- glm(Y_train ~ ., 
                  data = Train_Data, 
                  family = 'binomial')

summary(logistic)

predicted_log = predict(logistic, x_test, type = 'response')
```

## Decision Trees

```{r}
needs('rpart')

# grow tree
fit_dt <- rpart(y_train ~ ., data = train_data, method = "class")

summary(fit_dt)

# Predict Output
predicted_dt = predict(fit_dt, x_test)
```

## Support Vector Machine

```{r}
needs('e1071')

# Fitting model
fit_svm <- svm(y_train ~ ., data = train_data)

summary(fit_svm)

# Predict Output
predicted_svm = predict(fit_svm, x_test)
```

## Naive Bayes

```{r}
needs('e1071')

# Fitting model
fit_nb <- naiveBayes(y_train ~ ., data = train_data)

summary(fit_nb)

# Predict Output
predicted_nb = predict(fit_nb, x_test)
```

## kNN

```{r}
needs('class')

x <- cbind(x_train,y_train)
#Fitting model
fit_knn <- knn(y_train ~ ., data = train_data, k = 5)

summary(fit_knn)

#Predict Output
predicted_knn = predict(fit_knn, x_test)
```

## kMeans

```{r}
needs('cluster')

fit_km <- kmeans(X, 3)
#5 cluster solution
```

## Random Forest

```{r}
needs('randomForest')

#x <- cbind(x_train,y_train)
#Fitting model
fit_rf <- randomForest(y_train ~ .,  data = train_data, ntree = 500)

summary(fit_rf)

#Predict Output
predicted_rf = predict(fit_rf, x_test)
```

## Dimension Reduction Algorithms

```{r}
library(stats)

pca <- princomp(train, cor = TRUE)
train_reduced <- predict(pca,train)
test_reduced <- predict(pca,test)
```

## Gradient Boosting & Ada Boost

```{r}
needs('caret')

# Fitting model
fitControl <- trainControl(method = "repeatedcv",
                           number = 4, 
                           repeats = 4)

fit_boost <- train(y_train ~ ., data = train_data, 
                   method = "gbm",
                   trControl = fitControl,
                   verbose = FALSE)

predicted_boost = predict(fit_boost, 
                          x_test,
                          type = "raw")[,2]
```
